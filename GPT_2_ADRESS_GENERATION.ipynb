{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orZK4OSWy7eH"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "12Uhso4vzfSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "OdextLcQ1QFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "DKnFv-FY1hmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, re , pandas as  pd, json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers  import DataCollatorForLanguageModeling, DataCollatorWithPadding, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments,AutoConfig,TFGPT2LMHeadModel, TFTrainer\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "m9vKceayz19Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():  \n",
        "    dev = \"cuda:0\" \n",
        "else:  \n",
        "    dev = \"cpu\"  \n",
        "device = torch.device(dev)  "
      ],
      "metadata": {
        "id": "6i23GRf41qQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "GGbq94yy17jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL AND TOKENIZER LOADING"
      ],
      "metadata": {
        "id": "yQv97x4L2B2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model=GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "oCc6ByWO2FNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.num_parameters"
      ],
      "metadata": {
        "id": "_2L1uDMU2qJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "9HSFLNnc20Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPARATION"
      ],
      "metadata": {
        "id": "yuRau03x3lsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "09lNW_Ck3mZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_path = '/content/gdrive/My Drive/Datasets/'"
      ],
      "metadata": {
        "id": "3_-FE10F3wrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join(original_path)\n",
        "data_dir"
      ],
      "metadata": {
        "id": "IqV5yO5s3yfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_url=os.path.join(data_dir, 'big_data.json')"
      ],
      "metadata": {
        "id": "XWUFmZOy30YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_json(df_url)"
      ],
      "metadata": {
        "id": "iZcQ0yvW33Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_name='element'"
      ],
      "metadata": {
        "id": "1ErNXWW85X_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic={\n",
        "     'element':[],\n",
        "     \n",
        "     'address':[]\n",
        "}\n",
        "df=pd.DataFrame(dic)\n",
        "df\n"
      ],
      "metadata": {
        "id": "yufUZKbp37Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,i in enumerate(data.rows.copy()):\n",
        "  df.loc[k]=[i['element'], i['element']['address']]"
      ],
      "metadata": {
        "id": "LfwK2TSi39qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df['element']:\n",
        "    del i['address']"
      ],
      "metadata": {
        "id": "ICjwfR1t4Gt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "hvtPhfWs435Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attributes=[]\n",
        "for i in range(df.shape[0]):\n",
        "    attributes.extend( df.iloc[:][col_name][i].keys())"
      ],
      "metadata": {
        "id": "CsgZnV4g5SDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_mean={}\n",
        "for el in set(attributes):\n",
        "   if attributes.count(el) >= 100:\n",
        "      attr_mean[el]=attributes.count(el)"
      ],
      "metadata": {
        "id": "kePFrMIz5hVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(attr_mean.keys())"
      ],
      "metadata": {
        "id": "zb8HlUCj5l5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "empt_arr={}\n",
        "\n",
        "for key in list(attr_mean.keys()):\n",
        "  empt_arr[key]=''\n",
        " "
      ],
      "metadata": {
        "id": "FY0PdsDr5nD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir=df.copy()\n",
        "for i in range(df.shape[0]): \n",
        "        cont=df_edir.iloc[i][col_name].copy()\n",
        "        for key in df_edir.iloc[i][col_name].keys():\n",
        "         \n",
        "         if key not in list(attr_mean.keys()):\n",
        "            del cont[key]\n",
        "         \n",
        "         empt_arr2=empt_arr.copy()\n",
        "         empt_arr2.update(cont)\n",
        "         df_edir[col_name][i]=empt_arr2\n",
        "     "
      ],
      "metadata": {
        "id": "w9bOYDSz5xgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir.head()"
      ],
      "metadata": {
        "id": "NjjaNJpL7HYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,i in enumerate(data.rows.copy()):\n",
        "  df_edir['element'][k]=\" \".join(sorted(i['element'].values()))"
      ],
      "metadata": {
        "id": "6haA3c24FdYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir.head()"
      ],
      "metadata": {
        "id": "7EtEoPBGFjfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text_ids=[]"
      ],
      "metadata": {
        "id": "YGwtscFnEWP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION"
      ],
      "metadata": {
        "id": "t527icIcIAZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the eos and bos tokens are defined\n",
        "bos = '<|endoftext|>'\n",
        "eos = '<|EOS|>'\n",
        "pad = '<|pad|>'"
      ],
      "metadata": {
        "id": "r_SIIXukIDMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokends_dict={'eos_token':eos,\n",
        "                      'bos_token':bos,\n",
        "                      'pad_token':pad}"
      ],
      "metadata": {
        "id": "XL1qQHl7IF5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the new token is added to the tokenizer\n",
        "num_added_toks=base_tokenizer.add_special_tokens(special_tokends_dict)"
      ],
      "metadata": {
        "id": "lUqLXXJ2JmBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config=AutoConfig.from_pretrained('gpt2',\n",
        "                                  bos_token_id=base_tokenizer.bos_token_id,\n",
        "                                  eos_token_id=base_tokenizer.eos_token_id,\n",
        "                                  \n",
        "                                  pad_token_id=base_tokenizer.pad_token_id,\n",
        "                                    output_hidden_states=False)"
      ],
      "metadata": {
        "id": "2Qp2ABi9O0T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model=TFGPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "4M2l8n9UUPIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.resize_token_embeddings(len(base_tokenizer))\n"
      ],
      "metadata": {
        "id": "xV_fv1XcUmIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dd_toks=df_edir.copy()"
      ],
      "metadata": {
        "id": "Qcyiv2H0ftZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,i in enumerate(data.rows.copy()):\n",
        "    df_edir['element'][k]=bos+' ' +df_edir['element'][k]#+' '+eos"
      ],
      "metadata": {
        "id": "Qh5xc7_Df-ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,i in enumerate(data.rows.copy()):\n",
        "    df_edir['address'][k]=bos+' ' +df_edir['address'][k]#+' '+eos"
      ],
      "metadata": {
        "id": "ofSkD0SD-PGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir.head()"
      ],
      "metadata": {
        "id": "FnQt5sNh6VSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text_ids=[]\n",
        "df_addr_ids=[]"
      ],
      "metadata": {
        "id": "SbBNywv7rlJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,i in enumerate(data.rows.copy()):\n",
        "  df_text_ids.append((base_tokenizer.encode( df_edir['element'][k])))"
      ],
      "metadata": {
        "id": "zaHgWykPEcKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,i in enumerate(data.rows.copy()):\n",
        "  df_addr_ids.append((base_tokenizer.encode( df_edir['address'][k])))"
      ],
      "metadata": {
        "id": "3wYF1FJOuw1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "30K8jjjjFsGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir['text_ids']=df_text_ids"
      ],
      "metadata": {
        "id": "Z74JPVcUp9j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir['addr_ids']=df_addr_ids"
      ],
      "metadata": {
        "id": "mYYciD6Ou4_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir['addr_ids'].head()"
      ],
      "metadata": {
        "id": "uzETQLPNvPKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir['text_ids'].head()"
      ],
      "metadata": {
        "id": "p-r4Jw5OIRaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size=1000"
      ],
      "metadata": {
        "id": "eiqj9MbpTdm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "rkM2ndkBIs_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df_edir['text_ids'])):\n",
        "   df_edir['text_ids'][i]=np.array(df_edir['text_ids'][i])"
      ],
      "metadata": {
        "id": "7EccpqMTHwPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df_edir['addr_ids'])):\n",
        "   df_edir['addr_ids'][i]=np.array(df_edir['addr_ids'][i])"
      ],
      "metadata": {
        "id": "OVlQqf4p8j6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df_edir['text_ids'])):\n",
        "  df_edir['text_ids'][i]=(df_edir['text_ids'][i]).astype(np.float32)"
      ],
      "metadata": {
        "id": "ouV2ka5em-lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=tf.keras.preprocessing.sequence.pad_sequences(list(df_edir['text_ids'][:size].values),padding='post')"
      ],
      "metadata": {
        "id": "4Nl_yRInUD2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  import numpy as np\n"
      ],
      "metadata": {
        "id": "oBsQSvS51jFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_eos(data):\n",
        "  data=tf.keras.preprocessing.sequence.pad_sequences(data,padding='post')\n",
        "  new_data=[]\n",
        "  for i in range(len(data)):\n",
        "    new_data.append(np.append(data[i],50257))\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "OekjkWUyzs1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data =add_eos(train_data)"
      ],
      "metadata": {
        "id": "uzP5n7F20AZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_daata =add_eos(test_data)"
      ],
      "metadata": {
        "id": "APu96muF8wTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_daata[0]"
      ],
      "metadata": {
        "id": "ivwICyvD82ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=tf.keras.preprocessing.sequence.pad_sequences(list(df_edir['text_ids'][size:size+700].values),padding='post')"
      ],
      "metadata": {
        "id": "wmqxR55AUROM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir.head()"
      ],
      "metadata": {
        "id": "qus8BHTu_QA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE_PER_REPLICA = 28\n",
        "EPOCHS = 6\n",
        "INITAL_LEARNING_RATE = 0.001\n",
        "try:\n",
        "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "except NameError as e:\n",
        "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA\n",
        "BUFFER_SIZE = len(df_token)\n",
        "\n",
        "# prepare data for consumption\n",
        "train_ds = (\n",
        "   df_token.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        ")\n",
        "test_ds =df_token.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "Ei9H3bBnLAde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "id": "a-jjyWTmfsRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drecreasing learning rate scheduler\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    INITAL_LEARNING_RATE,\n",
        "    decay_steps=500,\n",
        "    decay_rate=0.7,\n",
        "    staircase=True)\n",
        "\n",
        "# initialize model, use_cache=False important! else wrong shape at loss calc\n",
        "with strategy.scope():\n",
        "    model = TFGPT2LMHeadModel.from_pretrained(\n",
        "        \"gpt2\",\n",
        "        use_cache=False,\n",
        "        pad_token_id=base_tokenizer.pad_token_id,\n",
        "        eos_token_id=base_tokenizer.eos_token_id,\n",
        "    )\n",
        "    model.resize_token_embeddings(len(base_tokenizer))\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    model.compile(optimizer=optimizer, loss=model.compute_loss)\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "VClnXQaTK8ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"input_ids\": train_data,\n",
        "        }\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "FFhwx3ZrBnvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "        {\"input_ids\": test_daata,\n",
        "        }\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "9fdf_hjpBw6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(\n",
        "    X=train_data,\n",
        "    y\n",
        "    validation_data=test_daata,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "WpwzZJconiBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddreesGeneration(Dataset):\n",
        "  def __init__(self, txt_list,label_list,tokenizer, max_length):\n",
        "            #define variables\n",
        "            self.input_ids=[]\n",
        "            self.attn_mask=[]\n",
        "            self.labels=[]\n",
        "            map_label=[]\n",
        "            #iterate through the dataset\n",
        "            for txt, label in zip(txt_list,label_list):\n",
        "                  #prepare the text\n",
        "                  prep_text=f'<startoftext>Element: {txt}\\n Address: {label}<endoftext>'\n",
        "                  #tokenize\n",
        "                  encodings_dict=tokenizer(prep_text,truncation=True,max_length=max_length,\n",
        "                                           padding='max_length')\n",
        "                  \n",
        "                  #append to list\n",
        "                  self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "                  self.attn_mask.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "                  self.labels.append(label)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.input_ids)\n",
        "\n",
        "  \n",
        "  def __getiten__(self,idx):\n",
        "      return self.input_ids[idx], self.attn_mask[idx], self.labels[id]"
      ],
      "metadata": {
        "id": "HQ3Y7wBgAsow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD MODEL AND DATA"
      ],
      "metadata": {
        "id": "Y4v2_lKrIhsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "set model name"
      ],
      "metadata": {
        "id": "TRqRgJXBIlFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"gpt2\""
      ],
      "metadata": {
        "id": "US2BKiisInYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEED"
      ],
      "metadata": {
        "id": "xIURR6Q0IscH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "xKxlk6ZtI4jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast"
      ],
      "metadata": {
        "id": "SfMqpaYaVFSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD TOKENIZER AND MODEL"
      ],
      "metadata": {
        "id": "NMFeZAP0JMOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=GPT2TokenizerFast.from_pretrained(model_name,bos_token='<startoftext>',eos_token='<endoftext>',pad_token='<pad>')"
      ],
      "metadata": {
        "id": "U5ZmlUf_JJQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GPT2LMHeadModel.from_pretrained(model_name).cuda()"
      ],
      "metadata": {
        "id": "RGm-mEMMKZfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "b9Ak2_viKTIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir.head()"
      ],
      "metadata": {
        "id": "RHJnh82ILVqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size=1000"
      ],
      "metadata": {
        "id": "hHr_sh68CiX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=df_edir[:size]"
      ],
      "metadata": {
        "id": "Q5yD6fbK8lJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset=df_edir[size:size+700]"
      ],
      "metadata": {
        "id": "PAbAD-CeCniy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size=1000"
      ],
      "metadata": {
        "id": "Sgm8qffoCReO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=AddreesGeneration(txt_list=train_dataset['element'],label_list=train_dataset['address'],tokenizer=tokenizer, max_length=300)"
      ],
      "metadata": {
        "id": "iFPKp5xr8ycU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data=AddreesGeneration(txt_list=test_dataset['element'],label_list=test_dataset['address'],tokenizer=tokenizer, max_length=300)"
      ],
      "metadata": {
        "id": "-MZu0nuOCraE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN"
      ],
      "metadata": {
        "id": "bRWQaFH6_f2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATING TRAINING ARGUMENTS"
      ],
      "metadata": {
        "id": "-1ks-hx4_rRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args=TrainingArguments(output_dir='results', num_train_epochs=2, logging_steps=10,\n",
        "                               save_strategy='epoch',\n",
        "                                per_device_train_batch_size=2,per_device_eval_batch_size=2,\n",
        "                                warmup_steps=100,weight_decay=0.01,logging_dir='logs')"
      ],
      "metadata": {
        "id": "a0hpOLEX_hz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "di1Nh-yaQkiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=pd.DataFrame(\n",
        "    {\n",
        "         'input_ids':train_data.input_ids,\n",
        "          'attention_mask':train_data.attn_mask,\n",
        "          'labels':train_data.labels\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "HejcZQkwPaIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "PQpDEIRiQqHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n"
      ],
      "metadata": {
        "id": "wwe83XVql7Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_edir.head()"
      ],
      "metadata": {
        "id": "v_ThNp0-p0Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "jeqoYr3YD1Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('train_data.txt',df_edir.values,fmt='%s')"
      ],
      "metadata": {
        "id": "lLrBu8f-Esja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z0L-h3x-DjfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "START TRAINING"
      ],
      "metadata": {
        "id": "tfuI3UTVCA9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Trainer(model=model,args=training_args,train_dataset=df_train[['input_ids','attention_mask']]).train()"
      ],
      "metadata": {
        "id": "6XCzMGj8CEow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "DUGBvyZ5dgyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "JlyTyacHFz5t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}